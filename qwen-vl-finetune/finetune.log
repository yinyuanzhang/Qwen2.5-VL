nohup: ignoring input
W0718 02:23:09.854000 8230 site-packages/torch/distributed/run.py:792] 
W0718 02:23:09.854000 8230 site-packages/torch/distributed/run.py:792] *****************************************
W0718 02:23:09.854000 8230 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0718 02:23:09.854000 8230 site-packages/torch/distributed/run.py:792] *****************************************
Â§öËøõÁ®ãÂêØÂä®ÊñπÊ≥ïÂ∑≤ËÆæÁΩÆ‰∏∫ 'spawn'„ÄÇ
[2025-07-18 02:23:18,005] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Â§öËøõÁ®ãÂêØÂä®ÊñπÊ≥ïÂ∑≤ËÆæÁΩÆ‰∏∫ 'spawn'„ÄÇ
[2025-07-18 02:23:18,308] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Â§öËøõÁ®ãÂêØÂä®ÊñπÊ≥ïÂ∑≤ËÆæÁΩÆ‰∏∫ 'spawn'„ÄÇ
[2025-07-18 02:23:18,524] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Â§öËøõÁ®ãÂêØÂä®ÊñπÊ≥ïÂ∑≤ËÆæÁΩÆ‰∏∫ 'spawn'„ÄÇ
[2025-07-18 02:23:18,650] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-18 02:23:19,373] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-07-18 02:23:19,575] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-07-18 02:23:19,576] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-07-18 02:23:19,859] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-07-18 02:23:19,934] [INFO] [comm.py:658:init_distributed] cdb=None
Loading model and processor with image segmentation support...
[2025-07-18 02:23:20,861] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-07-18 02:23:21,213] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-07-18 02:23:21,453] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-07-18 02:23:21,517] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Phoenix22:8401:8401 [0] NCCL INFO Bootstrap : Using eno2:192.168.2.2<0>
Phoenix22:8401:8401 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
Phoenix22:8401:8401 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
Phoenix22:8401:8401 [0] NCCL INFO NET/Plugin: Using internal network plugin.
Phoenix22:8401:8401 [0] NCCL INFO cudaDriverVersion 12010
NCCL version 2.21.5+cuda11.0
Phoenix22:8403:8403 [2] NCCL INFO cudaDriverVersion 12010
Phoenix22:8403:8403 [2] NCCL INFO Bootstrap : Using eno2:192.168.2.2<0>
Phoenix22:8403:8403 [2] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
Phoenix22:8403:8403 [2] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
Phoenix22:8403:8403 [2] NCCL INFO NET/Plugin: Using internal network plugin.
Phoenix22:8401:8401 [0] NCCL INFO Comm config Blocking set to 1
Phoenix22:8401:8616 [0] NCCL INFO NET/IB : No device found.
Phoenix22:8401:8616 [0] NCCL INFO NET/Socket : Using [0]eno2:192.168.2.2<0> [1]tailscale0:100.119.229.53<0> [2]vmnet1:172.16.62.1<0> [3]vmnet8:192.168.11.1<0> [4]vboxnet1:192.168.56.1<0> [5]veth4b30295:fe80::7085:9aff:fe84:ebfe%veth4b30295<0>
Phoenix22:8401:8616 [0] NCCL INFO Using non-device net plugin version 0
Phoenix22:8401:8616 [0] NCCL INFO Using network Socket
Phoenix22:8403:8403 [2] NCCL INFO Comm config Blocking set to 1
Phoenix22:8403:8619 [2] NCCL INFO NET/IB : No device found.
Phoenix22:8403:8619 [2] NCCL INFO NET/Socket : Using [0]eno2:192.168.2.2<0> [1]tailscale0:100.119.229.53<0> [2]vmnet1:172.16.62.1<0> [3]vmnet8:192.168.11.1<0> [4]vboxnet1:192.168.56.1<0> [5]veth4b30295:fe80::7085:9aff:fe84:ebfe%veth4b30295<0>
Phoenix22:8403:8619 [2] NCCL INFO Using non-device net plugin version 0
Phoenix22:8403:8619 [2] NCCL INFO Using network Socket
Phoenix22:8402:8402 [1] NCCL INFO cudaDriverVersion 12010
Phoenix22:8402:8402 [1] NCCL INFO Bootstrap : Using eno2:192.168.2.2<0>
Phoenix22:8402:8402 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
Phoenix22:8402:8402 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
Phoenix22:8402:8402 [1] NCCL INFO NET/Plugin: Using internal network plugin.
Phoenix22:8404:8404 [3] NCCL INFO cudaDriverVersion 12010
Phoenix22:8404:8404 [3] NCCL INFO Bootstrap : Using eno2:192.168.2.2<0>
Phoenix22:8404:8404 [3] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
Phoenix22:8404:8404 [3] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
Phoenix22:8404:8404 [3] NCCL INFO NET/Plugin: Using internal network plugin.
Phoenix22:8402:8402 [1] NCCL INFO Comm config Blocking set to 1
Phoenix22:8402:8620 [1] NCCL INFO NET/IB : No device found.
Phoenix22:8402:8620 [1] NCCL INFO NET/Socket : Using [0]eno2:192.168.2.2<0> [1]tailscale0:100.119.229.53<0> [2]vmnet1:172.16.62.1<0> [3]vmnet8:192.168.11.1<0> [4]vboxnet1:192.168.56.1<0> [5]veth4b30295:fe80::7085:9aff:fe84:ebfe%veth4b30295<0>
Phoenix22:8402:8620 [1] NCCL INFO Using non-device net plugin version 0
Phoenix22:8402:8620 [1] NCCL INFO Using network Socket
Phoenix22:8404:8404 [3] NCCL INFO Comm config Blocking set to 1
Phoenix22:8404:8621 [3] NCCL INFO NET/IB : No device found.
Phoenix22:8404:8621 [3] NCCL INFO NET/Socket : Using [0]eno2:192.168.2.2<0> [1]tailscale0:100.119.229.53<0> [2]vmnet1:172.16.62.1<0> [3]vmnet8:192.168.11.1<0> [4]vboxnet1:192.168.56.1<0> [5]veth4b30295:fe80::7085:9aff:fe84:ebfe%veth4b30295<0>
Phoenix22:8404:8621 [3] NCCL INFO Using non-device net plugin version 0
Phoenix22:8404:8621 [3] NCCL INFO Using network Socket
Phoenix22:8401:8616 [0] NCCL INFO ncclCommInitRank comm 0xa6aa35c0 rank 0 nranks 4 cudaDev 0 nvmlDev 4 busId 88000 commId 0xd1194eadc78ce54f - Init START
Phoenix22:8403:8619 [2] NCCL INFO ncclCommInitRank comm 0xa2fc3b00 rank 2 nranks 4 cudaDev 2 nvmlDev 6 busId b1000 commId 0xd1194eadc78ce54f - Init START
Phoenix22:8404:8621 [3] NCCL INFO ncclCommInitRank comm 0xa5401690 rank 3 nranks 4 cudaDev 3 nvmlDev 7 busId b2000 commId 0xd1194eadc78ce54f - Init START
Phoenix22:8402:8620 [1] NCCL INFO ncclCommInitRank comm 0xa74fc9c0 rank 1 nranks 4 cudaDev 1 nvmlDev 5 busId 89000 commId 0xd1194eadc78ce54f - Init START
Phoenix22:8404:8621 [3] NCCL INFO Setting affinity for GPU 7 to ffff0000,ffff0000
Phoenix22:8401:8616 [0] NCCL INFO Setting affinity for GPU 4 to ffff0000,ffff0000
Phoenix22:8402:8620 [1] NCCL INFO Setting affinity for GPU 5 to ffff0000,ffff0000
Phoenix22:8403:8619 [2] NCCL INFO Setting affinity for GPU 6 to ffff0000,ffff0000
Phoenix22:8402:8620 [1] NCCL INFO comm 0xa74fc9c0 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
Phoenix22:8401:8616 [0] NCCL INFO comm 0xa6aa35c0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
Phoenix22:8401:8616 [0] NCCL INFO Channel 00/02 :    0   1   2   3
Phoenix22:8402:8620 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
Phoenix22:8403:8619 [2] NCCL INFO comm 0xa2fc3b00 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
Phoenix22:8404:8621 [3] NCCL INFO comm 0xa5401690 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
Phoenix22:8401:8616 [0] NCCL INFO Channel 01/02 :    0   1   2   3
Phoenix22:8402:8620 [1] NCCL INFO P2P Chunksize set to 131072
Phoenix22:8401:8616 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
Phoenix22:8403:8619 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
Phoenix22:8401:8616 [0] NCCL INFO P2P Chunksize set to 131072
Phoenix22:8404:8621 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
Phoenix22:8403:8619 [2] NCCL INFO P2P Chunksize set to 131072
Phoenix22:8404:8621 [3] NCCL INFO P2P Chunksize set to 131072
Phoenix22:8404:8621 [3] NCCL INFO Channel 00 : 3[7] -> 0[4] via SHM/direct/direct
Phoenix22:8402:8620 [1] NCCL INFO Channel 00 : 1[5] -> 2[6] via SHM/direct/direct
Phoenix22:8402:8620 [1] NCCL INFO Channel 01 : 1[5] -> 2[6] via SHM/direct/direct
Phoenix22:8404:8621 [3] NCCL INFO Channel 01 : 3[7] -> 0[4] via SHM/direct/direct
Phoenix22:8401:8616 [0] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
Phoenix22:8401:8616 [0] NCCL INFO Channel 01/0 : 0[4] -> 1[5] via P2P/IPC
Phoenix22:8403:8619 [2] NCCL INFO Channel 00/0 : 2[6] -> 3[7] via P2P/IPC
Phoenix22:8403:8619 [2] NCCL INFO Channel 01/0 : 2[6] -> 3[7] via P2P/IPC
Phoenix22:8403:8619 [2] NCCL INFO Connected all rings
Phoenix22:8404:8621 [3] NCCL INFO Connected all rings
Phoenix22:8401:8616 [0] NCCL INFO Connected all rings
Phoenix22:8404:8621 [3] NCCL INFO Channel 00/0 : 3[7] -> 2[6] via P2P/IPC
Phoenix22:8402:8620 [1] NCCL INFO Connected all rings
Phoenix22:8404:8621 [3] NCCL INFO Channel 01/0 : 3[7] -> 2[6] via P2P/IPC
Phoenix22:8403:8619 [2] NCCL INFO Channel 00 : 2[6] -> 1[5] via SHM/direct/direct
Phoenix22:8403:8619 [2] NCCL INFO Channel 01 : 2[6] -> 1[5] via SHM/direct/direct
Phoenix22:8402:8620 [1] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
Phoenix22:8402:8620 [1] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
Phoenix22:8401:8616 [0] NCCL INFO Connected all trees
Phoenix22:8401:8616 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
Phoenix22:8401:8616 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
Phoenix22:8402:8620 [1] NCCL INFO Connected all trees
Phoenix22:8402:8620 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
Phoenix22:8402:8620 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
Phoenix22:8404:8621 [3] NCCL INFO Connected all trees
Phoenix22:8403:8619 [2] NCCL INFO Connected all trees
Phoenix22:8404:8621 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
Phoenix22:8403:8619 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
Phoenix22:8404:8621 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
Phoenix22:8403:8619 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
Phoenix22:8404:8621 [3] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
Phoenix22:8404:8621 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
Phoenix22:8402:8620 [1] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
Phoenix22:8403:8619 [2] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
Phoenix22:8404:8621 [3] NCCL INFO ncclCommInitRank comm 0xa5401690 rank 3 nranks 4 cudaDev 3 nvmlDev 7 busId b2000 commId 0xd1194eadc78ce54f - Init COMPLETE
Phoenix22:8401:8616 [0] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
Phoenix22:8402:8620 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
Phoenix22:8403:8619 [2] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
Phoenix22:8401:8616 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
Phoenix22:8402:8620 [1] NCCL INFO ncclCommInitRank comm 0xa74fc9c0 rank 1 nranks 4 cudaDev 1 nvmlDev 5 busId 89000 commId 0xd1194eadc78ce54f - Init COMPLETE
Phoenix22:8403:8619 [2] NCCL INFO ncclCommInitRank comm 0xa2fc3b00 rank 2 nranks 4 cudaDev 2 nvmlDev 6 busId b1000 commId 0xd1194eadc78ce54f - Init COMPLETE
Phoenix22:8401:8616 [0] NCCL INFO ncclCommInitRank comm 0xa6aa35c0 rank 0 nranks 4 cudaDev 0 nvmlDev 4 busId 88000 commId 0xd1194eadc78ce54f - Init COMPLETE
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][2025-07-18 02:23:25,858] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1215, num_elems = 4.73B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  2.58s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  2.69s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  2.69s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  2.73s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.20s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.20s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.27s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.27s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.16s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.22s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.23s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.31s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Vision Module - Attention Blocks:
Trainable Block Indices: None
Non-Trainable Block Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
Merger Module Trainable: True
LLM Module - Embed Tokens Trainable: True
LLM Module - Trainable Layer Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
LLM Module - Non-Trainable Layer Indices: None
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Parameter Offload: Total persistent parameters: 755712 in 408 params
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: yinyuanzhang16 (yinyuanzhang16-peking-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/zyy/Qwen2.5-VL/qwen-vl-finetune/wandb/run-20250718_022411-knbqbvvg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run qwen2vl-baseline
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yinyuanzhang16-peking-university/huggingface
wandb: üöÄ View run at https://wandb.ai/yinyuanzhang16-peking-university/huggingface/runs/knbqbvvg
  0%|          | 0/4880 [00:00<?, ?it/s]/data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Phoenix22:8401:9951 [0] NCCL INFO Comm config Blocking set to 1
Phoenix22:8402:9960 [1] NCCL INFO Comm config Blocking set to 1
Phoenix22:8404:9967 [3] NCCL INFO Comm config Blocking set to 1
Phoenix22:8401:9969 [0] NCCL INFO Using non-device net plugin version 0
Phoenix22:8401:9969 [0] NCCL INFO Using network Socket
Phoenix22:8402:9970 [1] NCCL INFO Using non-device net plugin version 0
Phoenix22:8402:9970 [1] NCCL INFO Using network Socket
Phoenix22:8404:9971 [3] NCCL INFO Using non-device net plugin version 0
Phoenix22:8404:9971 [3] NCCL INFO Using network Socket
Phoenix22:8403:9957 [2] NCCL INFO Comm config Blocking set to 1
Phoenix22:8403:9972 [2] NCCL INFO Using non-device net plugin version 0
Phoenix22:8403:9972 [2] NCCL INFO Using network Socket
Phoenix22:8403:9972 [2] NCCL INFO ncclCommInitRank comm 0x7fa0a835c4d0 rank 2 nranks 4 cudaDev 2 nvmlDev 6 busId b1000 commId 0x9cd35fa594750129 - Init START
Phoenix22:8404:9971 [3] NCCL INFO ncclCommInitRank comm 0x7eff6c35b380 rank 3 nranks 4 cudaDev 3 nvmlDev 7 busId b2000 commId 0x9cd35fa594750129 - Init START
Phoenix22:8402:9970 [1] NCCL INFO ncclCommInitRank comm 0x7efc7c35c230 rank 1 nranks 4 cudaDev 1 nvmlDev 5 busId 89000 commId 0x9cd35fa594750129 - Init START
Phoenix22:8401:9969 [0] NCCL INFO ncclCommInitRank comm 0x7fe69435bd30 rank 0 nranks 4 cudaDev 0 nvmlDev 4 busId 88000 commId 0x9cd35fa594750129 - Init START
Phoenix22:8403:9972 [2] NCCL INFO Setting affinity for GPU 6 to ffff0000,ffff0000
Phoenix22:8404:9971 [3] NCCL INFO Setting affinity for GPU 7 to ffff0000,ffff0000
Phoenix22:8401:9969 [0] NCCL INFO Setting affinity for GPU 4 to ffff0000,ffff0000
Phoenix22:8402:9970 [1] NCCL INFO Setting affinity for GPU 5 to ffff0000,ffff0000
Phoenix22:8403:9972 [2] NCCL INFO comm 0x7fa0a835c4d0 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
Phoenix22:8402:9970 [1] NCCL INFO comm 0x7efc7c35c230 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
Phoenix22:8401:9969 [0] NCCL INFO comm 0x7fe69435bd30 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
Phoenix22:8403:9972 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
Phoenix22:8402:9970 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
Phoenix22:8403:9972 [2] NCCL INFO P2P Chunksize set to 131072
Phoenix22:8402:9970 [1] NCCL INFO P2P Chunksize set to 131072
Phoenix22:8401:9969 [0] NCCL INFO Channel 00/02 :    0   1   2   3
Phoenix22:8401:9969 [0] NCCL INFO Channel 01/02 :    0   1   2   3
Phoenix22:8404:9971 [3] NCCL INFO comm 0x7eff6c35b380 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
Phoenix22:8401:9969 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
Phoenix22:8401:9969 [0] NCCL INFO P2P Chunksize set to 131072
Phoenix22:8404:9971 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
Phoenix22:8404:9971 [3] NCCL INFO P2P Chunksize set to 131072
Phoenix22:8402:9970 [1] NCCL INFO Channel 00 : 1[5] -> 2[6] via SHM/direct/direct
Phoenix22:8402:9970 [1] NCCL INFO Channel 01 : 1[5] -> 2[6] via SHM/direct/direct
Phoenix22:8404:9971 [3] NCCL INFO Channel 00 : 3[7] -> 0[4] via SHM/direct/direct
Phoenix22:8404:9971 [3] NCCL INFO Channel 01 : 3[7] -> 0[4] via SHM/direct/direct
Phoenix22:8403:9972 [2] NCCL INFO Channel 00/0 : 2[6] -> 3[7] via P2P/IPC
Phoenix22:8401:9969 [0] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
Phoenix22:8403:9972 [2] NCCL INFO Channel 01/0 : 2[6] -> 3[7] via P2P/IPC
Phoenix22:8401:9969 [0] NCCL INFO Channel 01/0 : 0[4] -> 1[5] via P2P/IPC
Phoenix22:8403:9972 [2] NCCL INFO Connected all rings
Phoenix22:8401:9969 [0] NCCL INFO Connected all rings
Phoenix22:8404:9971 [3] NCCL INFO Connected all rings
Phoenix22:8402:9970 [1] NCCL INFO Connected all rings
Phoenix22:8404:9971 [3] NCCL INFO Channel 00/0 : 3[7] -> 2[6] via P2P/IPC
Phoenix22:8404:9971 [3] NCCL INFO Channel 01/0 : 3[7] -> 2[6] via P2P/IPC
Phoenix22:8403:9972 [2] NCCL INFO Channel 00 : 2[6] -> 1[5] via SHM/direct/direct
Phoenix22:8403:9972 [2] NCCL INFO Channel 01 : 2[6] -> 1[5] via SHM/direct/direct
Phoenix22:8402:9970 [1] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
Phoenix22:8402:9970 [1] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
Phoenix22:8401:9969 [0] NCCL INFO Connected all trees
Phoenix22:8401:9969 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
Phoenix22:8401:9969 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
Phoenix22:8402:9970 [1] NCCL INFO Connected all trees
Phoenix22:8402:9970 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
Phoenix22:8402:9970 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
Phoenix22:8403:9972 [2] NCCL INFO Connected all trees
Phoenix22:8404:9971 [3] NCCL INFO Connected all trees
Phoenix22:8403:9972 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
Phoenix22:8403:9972 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
Phoenix22:8404:9971 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
Phoenix22:8404:9971 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
Phoenix22:8404:9971 [3] NCCL INFO ncclCommInitRank comm 0x7eff6c35b380 rank 3 nranks 4 cudaDev 3 nvmlDev 7 busId b2000 commId 0x9cd35fa594750129 - Init COMPLETE
Phoenix22:8402:9970 [1] NCCL INFO ncclCommInitRank comm 0x7efc7c35c230 rank 1 nranks 4 cudaDev 1 nvmlDev 5 busId 89000 commId 0x9cd35fa594750129 - Init COMPLETE
Phoenix22:8403:9972 [2] NCCL INFO ncclCommInitRank comm 0x7fa0a835c4d0 rank 2 nranks 4 cudaDev 2 nvmlDev 6 busId b1000 commId 0x9cd35fa594750129 - Init COMPLETE
Phoenix22:8401:9969 [0] NCCL INFO ncclCommInitRank comm 0x7fe69435bd30 rank 0 nranks 4 cudaDev 0 nvmlDev 4 busId 88000 commId 0x9cd35fa594750129 - Init COMPLETE
  0%|          | 1/4880 [02:19<189:35:55, 139.90s/it]                                                     {'loss': 0.9656, 'grad_norm': 3.68004725009824, 'learning_rate': 1.3605442176870747e-09, 'epoch': 0.0}
  0%|          | 1/4880 [02:19<189:35:55, 139.90s/it]  0%|          | 2/4880 [02:31<86:59:10, 64.20s/it]                                                     {'loss': 1.0469, 'grad_norm': 3.7342232600833536, 'learning_rate': 2.7210884353741494e-09, 'epoch': 0.0}
  0%|          | 2/4880 [02:31<86:59:10, 64.20s/it]  0%|          | 3/4880 [02:42<54:12:03, 40.01s/it]                                                   {'loss': 1.0027, 'grad_norm': 4.169149409236992, 'learning_rate': 4.081632653061224e-09, 'epoch': 0.0}
  0%|          | 3/4880 [02:42<54:12:03, 40.01s/it]  0%|          | 4/4880 [02:53<38:45:36, 28.62s/it]                                                   {'loss': 1.2183, 'grad_norm': 6.511745164693661, 'learning_rate': 5.442176870748299e-09, 'epoch': 0.0}
  0%|          | 4/4880 [02:53<38:45:36, 28.62s/it][rank2]:[E718 02:37:10.861385790 ProcessGroupNCCL.cpp:629] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5007, OpType=_REDUCE_SCATTER_BASE, NumelIn=2048, NumelOut=512, Timeout(ms)=600000) ran for 600017 milliseconds before timing out.
[rank2]:[E718 02:37:10.861734863 ProcessGroupNCCL.cpp:2168] [PG ID 1 PG GUID 1 Rank 2]  failure detected by watchdog at work sequence id: 5007 PG status: last enqueued work: 5009, last completed work: 5006
[rank0]:[E718 02:37:10.889368655 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5007, OpType=_REDUCE_SCATTER_BASE, NumelIn=2048, NumelOut=512, Timeout(ms)=600000) ran for 600047 milliseconds before timing out.
[rank0]:[E718 02:37:10.889536028 ProcessGroupNCCL.cpp:2168] [PG ID 1 PG GUID 1 Rank 0]  failure detected by watchdog at work sequence id: 5007 PG status: last enqueued work: 5009, last completed work: 5006
[rank2]:[E718 02:37:10.889784260 ProcessGroupNCCL.cpp:664] Stack trace of the failed collective: 
#0 reduce_scatter_tensor from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4235
#1 wrapper from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:81
#2 reduce_scatter_tensor from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/comm/torch.py:274
#3 reduce_scatter_tensor from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/comm/comm.py:295
#4 log_wrapper from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/comm/comm.py:117
#5 reduce_scatter_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/comm/comm.py:263
#6 wrapped_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18
#7 _torch_reduce_scatter_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/comm/coalesced_collectives.py:23
#8 reduce_scatter_coalesced from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/comm/coalesced_collectives.py:205
#9 decorate_context from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/utils/_contextlib.py:116
#10 wrapped_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18
#11 __avg_scatter_grads from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1391
#12 wrapped_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18
#13 __reduce_and_partition_ipg_grads from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1319
#14 decorate_context from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/utils/_contextlib.py:116
#15 wrapped_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18
#16 reduce_independent_p_g_buckets_and_remove_grads from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1269
#17 reduce_ready_partitions_and_remove_grads from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1531
#18 reduce_partition_and_remove_grads from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1178
#19 wrapped_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18
#20 _engine_run_backward from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/autograd/graph.py:823
#21 backward from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/autograd/__init__.py:347
#22 backward from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/utils/checkpoint.py:321
#23 apply from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/autograd/function.py:307

[rank0]:[E718 02:37:10.890603925 ProcessGroupNCCL.cpp:664] Stack trace of the failed collective: 
#0 reduce_scatter_tensor from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4235
#1 wrapper from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:81
#2 reduce_scatter_tensor from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/comm/torch.py:274
#3 reduce_scatter_tensor from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/comm/comm.py:295
#4 log_wrapper from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/comm/comm.py:117
#5 reduce_scatter_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/comm/comm.py:263
#6 wrapped_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18
#7 _torch_reduce_scatter_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/comm/coalesced_collectives.py:23
#8 reduce_scatter_coalesced from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/comm/coalesced_collectives.py:205
#9 decorate_context from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/utils/_contextlib.py:116
#10 wrapped_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18
#11 __avg_scatter_grads from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1391
#12 wrapped_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18
#13 __reduce_and_partition_ipg_grads from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1319
#14 decorate_context from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/utils/_contextlib.py:116
#15 wrapped_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18
#16 reduce_independent_p_g_buckets_and_remove_grads from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1269
#17 reduce_ready_partitions_and_remove_grads from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1531
#18 reduce_partition_and_remove_grads from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1178
#19 wrapped_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18
#20 _engine_run_backward from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/autograd/graph.py:823
#21 backward from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/autograd/__init__.py:347
#22 backward from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/utils/checkpoint.py:321
#23 apply from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/autograd/function.py:307

[rank3]:[E718 02:37:10.936724144 ProcessGroupNCCL.cpp:629] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5007, OpType=_REDUCE_SCATTER_BASE, NumelIn=2048, NumelOut=512, Timeout(ms)=600000) ran for 600064 milliseconds before timing out.
[rank3]:[E718 02:37:10.937068503 ProcessGroupNCCL.cpp:2168] [PG ID 1 PG GUID 1 Rank 3]  failure detected by watchdog at work sequence id: 5007 PG status: last enqueued work: 5009, last completed work: 5006
[rank3]:[E718 02:37:10.938116758 ProcessGroupNCCL.cpp:664] Stack trace of the failed collective: 
#0 reduce_scatter_tensor from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4235
#1 wrapper from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:81
#2 reduce_scatter_tensor from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/comm/torch.py:274
#3 reduce_scatter_tensor from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/comm/comm.py:295
#4 log_wrapper from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/comm/comm.py:117
#5 reduce_scatter_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/comm/comm.py:263
#6 wrapped_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18
#7 _torch_reduce_scatter_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/comm/coalesced_collectives.py:23
#8 reduce_scatter_coalesced from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/comm/coalesced_collectives.py:205
#9 decorate_context from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/utils/_contextlib.py:116
#10 wrapped_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18
#11 __avg_scatter_grads from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1391
#12 wrapped_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18
#13 __reduce_and_partition_ipg_grads from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1319
#14 decorate_context from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/utils/_contextlib.py:116
#15 wrapped_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18
#16 reduce_independent_p_g_buckets_and_remove_grads from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1269
#17 reduce_ready_partitions_and_remove_grads from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1531
#18 reduce_partition_and_remove_grads from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1178
#19 wrapped_fn from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18
#20 _engine_run_backward from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/autograd/graph.py:823
#21 backward from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/autograd/__init__.py:347
#22 backward from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/utils/checkpoint.py:321
#23 apply from /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/autograd/function.py:307

Phoenix22:8401:9975 [0] NCCL INFO [Service thread] Connection closed by localRank 0
Phoenix22:8404:9979 [3] NCCL INFO [Service thread] Connection closed by localRank 3
Phoenix22:8401:8879 [0] NCCL INFO comm 0x7fe69435bd30 rank 0 nranks 4 cudaDev 0 busId 88000 - Abort COMPLETE
[rank0]:[E718 02:37:10.735061989 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E718 02:37:10.735076401 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
Phoenix22:8404:8884 [3] NCCL INFO comm 0x7eff6c35b380 rank 3 nranks 4 cudaDev 3 busId b2000 - Abort COMPLETE
[rank3]:[E718 02:37:10.741412299 ProcessGroupNCCL.cpp:681] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E718 02:37:10.741429306 ProcessGroupNCCL.cpp:695] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E718 02:37:10.769967259 ProcessGroupNCCL.cpp:1895] [PG ID 1 PG GUID 1 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5007, OpType=_REDUCE_SCATTER_BASE, NumelIn=2048, NumelOut=512, Timeout(ms)=600000) ran for 600064 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f01565451b6 in /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f01577c7e14 in /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f01577c9970 in /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f01577ca88d in /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f01b052f5c0 in /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94b43 (0x7f01b165db43 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a00 (0x7f01b16efa00 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E718 02:37:10.771501037 ProcessGroupNCCL.cpp:1895] [PG ID 1 PG GUID 1 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5007, OpType=_REDUCE_SCATTER_BASE, NumelIn=2048, NumelOut=512, Timeout(ms)=600000) ran for 600047 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fe8f41451b6 in /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fe8f53c7e14 in /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fe8f53c9970 in /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fe8f53ca88d in /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fe94e0f55c0 in /data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94b43 (0x7fe94f223b43 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a00 (0x7fe94f2b5a00 in /lib/x86_64-linux-gnu/libc.so.6)

Â§öËøõÁ®ãÂêØÂä®ÊñπÊ≥ïÂ∑≤ËÆæÁΩÆ‰∏∫ 'spawn'„ÄÇ
Worker loading YOLO model on GPU device: cuda:3 (physical ID depends on CUDA_VISIBLE_DEVICES)
Â§öËøõÁ®ãÂêØÂä®ÊñπÊ≥ïÂ∑≤ËÆæÁΩÆ‰∏∫ 'spawn'„ÄÇ
Worker loading YOLO model on GPU device: cuda:0 (physical ID depends on CUDA_VISIBLE_DEVICES)
Â§öËøõÁ®ãÂêØÂä®ÊñπÊ≥ïÂ∑≤ËÆæÁΩÆ‰∏∫ 'spawn'„ÄÇ
Worker loading YOLO model on GPU device: cuda:0 (physical ID depends on CUDA_VISIBLE_DEVICES)
Â§öËøõÁ®ãÂêØÂä®ÊñπÊ≥ïÂ∑≤ËÆæÁΩÆ‰∏∫ 'spawn'„ÄÇ
Worker loading YOLO model on GPU device: cuda:3 (physical ID depends on CUDA_VISIBLE_DEVICES)
Â§öËøõÁ®ãÂêØÂä®ÊñπÊ≥ïÂ∑≤ËÆæÁΩÆ‰∏∫ 'spawn'„ÄÇ
Worker loading YOLO model on GPU device: cuda:0 (physical ID depends on CUDA_VISIBLE_DEVICES)
W0718 02:37:12.024000 8230 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 8402 closing signal SIGTERM
W0718 02:37:12.030000 8230 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 8403 closing signal SIGTERM
Â§öËøõÁ®ãÂêØÂä®ÊñπÊ≥ïÂ∑≤ËÆæÁΩÆ‰∏∫ 'spawn'„ÄÇ
Worker loading YOLO model on GPU device: cuda:3 (physical ID depends on CUDA_VISIBLE_DEVICES)
Â§öËøõÁ®ãÂêØÂä®ÊñπÊ≥ïÂ∑≤ËÆæÁΩÆ‰∏∫ 'spawn'„ÄÇ
Worker loading YOLO model on GPU device: cuda:3 (physical ID depends on CUDA_VISIBLE_DEVICES)
Â§öËøõÁ®ãÂêØÂä®ÊñπÊ≥ïÂ∑≤ËÆæÁΩÆ‰∏∫ 'spawn'„ÄÇ
Worker loading YOLO model on GPU device: cuda:0 (physical ID depends on CUDA_VISIBLE_DEVICES)
/data/yinyuan/conda_env/qwen2/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 21 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/data/yinyuan/conda_env/qwen2/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 21 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
E0718 02:37:13.620000 8230 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 0 (pid: 8401) of binary: /data/yinyuan/conda_env/qwen2/bin/python
Traceback (most recent call last):
  File "/data/yinyuan/conda_env/qwen2/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/yinyuan/conda_env/qwen2/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=====================================================
qwenvl/train/train_qwen.py FAILED
-----------------------------------------------------
Failures:
[1]:
  time      : 2025-07-18_02:37:12
  host      : phoenix22.tail12dc50.ts.net
  rank      : 3 (local_rank: 3)
  exitcode  : -6 (pid: 8404)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 8404
-----------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-18_02:37:12
  host      : phoenix22.tail12dc50.ts.net
  rank      : 0 (local_rank: 0)
  exitcode  : -6 (pid: 8401)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 8401
=====================================================
/data/yinyuan/conda_env/qwen2/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 21 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/data/yinyuan/conda_env/qwen2/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 21 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
